#!/usr/bin/env python
# coding: utf-8

# # Case Petz vaga de cientista de dados Jr

# # An√°lise de Interna√ß√µes no Sistema de Sa√∫de Brasileiro

# **Introdu√ß√£o**
# 
# Voc√™ foi contratado(a) para fazer uma an√°lise apurada do n√∫mero de interna√ß√µes no
# sistema de sa√∫de brasileiro. Esta an√°lise √© de extrema import√¢ncia para tomada de
# decis√µes que dever√£o contribuir para melhorias no sistema e planejamento estrat√©gico.
# Os dados em anexo (case_internacao_SUS.xls) s√£o referentes √†s interna√ß√µes que
# ocorreram no pa√≠s durante o per√≠odo de dezembro de 2017 a julho de 2019, separados
# por regi√£o e unidade de federa√ß√£o (Fonte: Minist√©rio da Sa√∫de - Sistema de Informa√ß√µes
# Hospitalares do SUS (SIH/SUS)).

# **Base de dados**
# 
# - Link: http://tabnet.datasus.gov.br/cgi/sih/sxdescr.htm 

# **Tratamento dos dados**
# 
# 
# - 1: Muitas vezes, cerca de 70% do tempo de um projeto √© despendido na coleta e tratamento dos dados. Sabendo disso, leia o arquivo e o transforme de modo a ter mais facilidade em analisar os dados. Lembre-se que essa etapa poder√° te dar bons insumos.
# 
# 
# 
# **An√°lise**
# 
# 
# - 2: Dados tratados, bora explor√°-los? Fa√ßa uma boa EDA e n√£o esque√ßa de anotar todos os insights que voc√™ obter. Gr√°ficos e informa√ß√µes sem uma boa interpreta√ß√£o n√£o valem, ok?
# 
# 
# 
# **Modelagem**
# 
# 
# - 3: Agora que j√° tem certa intimidade com os dados, cite pelo menos 2 m√©todos poss√≠veis para estimar os dados para os meses faltantes. Tente n√£o se complicar aqui. Utilize os m√©todos mais simples e mais funcionais poss√≠veis. Neste t√≥pico, √© importante que argumente o porqu√™ dos m√©todos recomendados.Escolha um desses m√©todos e estime. 
# 
# 
# - a) o n√∫mero de Interna√ß√µes.
# 
# - b) o Valor Total das interna√ß√µes nos per√≠odos faltantes.
# 
# 
# 
# **Crie um modelo que preveja** 
# 
# 
# - a) As Interna√ß√µes.
# 
# 
# - b) O n√∫mero de √ìbitos.
# 
# 
# - c) O Valor M√©dio de AIH pelos pr√≥ximos 6 meses. 
# 
# Explique a escolha do modelo e quais par√¢metros utilizou para serem input no modelo.
# 
# 
# 
# **Planejamento estrat√©gico**
# 
# - Com base nos dados e nas suas an√°lises, que tipo de estrat√©gia voc√™ sugeriria para diminuir o n√∫mero de interna√ß√µes em hospitais do SUS? E para o Estado de S√£o Paulo? Quais especificidades deveriam ser levadas em conta?
# 

# # 0 - Importa√ß√£o das bibliotecas

# In[111]:


# Vers√£o do python
from platform import python_version

print('Vers√£o python neste Jupyter Notebook:', python_version())


# In[112]:


# Importa√ß√£o das bibliotecas 

import pandas as pd # Pandas carregamento csv
import numpy as np # Numpy para carregamento c√°lculos em arrays multidimensionais

# Visualiza√ß√£o de dados
import seaborn as sns
import matplotlib as m
import matplotlib as mpl
import matplotlib.pyplot as plt
import plotly
import plotly.express as px

# Carregar as vers√µes das bibliotecas
import watermark

# Warnings retirar alertas 
import warnings
warnings.filterwarnings("ignore")


# In[113]:


# Vers√µes das bibliotecas

get_ipython().run_line_magic('reload_ext', 'watermark')
get_ipython().run_line_magic('watermark', '-a "Vers√µes das bibliotecas" --iversions')


# In[114]:


# Configura√ß√£o para os gr√°ficos largura e layout dos graficos

plt.rcParams["figure.figsize"] = (25, 20)

plt.style.use('fivethirtyeight')
pd.set_option('display.expand_frame_repr', False)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

m.rcParams['axes.labelsize'] = 25
m.rcParams['xtick.labelsize'] = 25
m.rcParams['ytick.labelsize'] = 25
m.rcParams['text.color'] = 'k'


# # 0.1) Base de dados

# In[115]:


# Carregando a base de dados
base = pd.read_excel('case_internacao_SUS.xls', sheet_name=None)


# In[116]:


base_1.head()


# # 0.2) Descri√ß√£o dados
# 
# - Verifica√ß√£o de linhas colunas informa√ß√£os dos dados e tipos de vari√°veis. Valores das colunas verficando dados nulos ou vazios.

# In[117]:


# Exibido 5 primeiros dados
base_1.head()


# In[118]:


# Exibido 5 √∫ltimos dados 
base_1.tail()


# In[119]:


# N√∫mero de linhas e colunas
base_1.shape


# In[120]:


# Verificando informa√ß√µes das variaveis
base_1.info()


# In[121]:


# Exibido tipos de dados
base_1.dtypes


# In[122]:


# Total de colunas e linhas 

print("N√∫meros de linhas: {}" .format(base_1.shape[0]))
print("N√∫meros de colunas: {}" .format(base_1.shape[1]))


# In[123]:


# Exibindo valores ausentes e valores √∫nicos

print("\nMissing values :  ", base_1.isnull().sum().values.sum())
print("\nUnique values :  \n",base_1.nunique())


# # 0.3) Verifica√ß√£o dos dados
# 
# 

# In[124]:


# C√≥pia de seguran√ßa dos dados
data_1 = base_1.copy()

# Renomeando colunas
n_1 = data_1.columns
n_2 = lambda x: x.lower()
base = list(map(n_2, n_1))

data_1.columns = base
data_1.columns = ["Regi√£o",
                  "Interna√ß√µes",
                  "AIH_aprovadas",
                  "Valor_total",
                  "Valor_servi√ßos_hospitalares",
                  "Val_serv_hosp_compl_federal",
                  "Val_serv_hosp_compl_gestor",
                  "Valor_servi√ßos_profissionais",
                  "Val_serv_prof_compl_federal",
                  "Val_serv_prof_compl_gestor",
                  "Valor_m√©dio_AIH",
                  "Valor_m√©dio_intern",
                  "Dias_perman√™ncia",
                  "M√©dia_perman√™ncia",
                  "√ìbitos",
                  "Taxa_mortalidade",
                  "Data"]

data_1.head()


# In[125]:


# Dados faltantes coluna √≥bitos

data = data_1[data_1["√ìbitos"].notnull()]
data.isna().sum()


# In[126]:


# Dados faltantes colunas internacoes

data = data_1[data_1["Interna√ß√µes"].notnull()]
data.isna().sum()


# In[127]:


# Removendo dados ausentes do dataset 

data_1 = data_1.dropna()
data_1.head()


# In[128]:


# Sum() Retorna a soma dos valores sobre o eixo solicitado
# Isna() Detecta valores ausentes

data_1.isna().sum()


# In[129]:


# Retorna a soma dos valores sobre o eixo solicitado
# Detecta valores n√£o ausentes para um objeto semelhante a uma matriz.

data_1.notnull().sum()


# In[130]:


# Total de n√∫mero duplicados

data_1.duplicated()


# In[131]:


# Renomeando estados por regi√£o 

data_1["Regi√£o"].unique()


# In[132]:


# Regi√£oes que t√™m pontos(.) antes dos nomes 

data_1 = data_1[data_1['Regi√£o'].str.contains('.', regex=False)]
data_1['Regi√£o'].unique()


# In[133]:


# Estados vazios

data_1[data_1["Regi√£o"].isnull()]


# In[134]:


# Estados vazios um filtro de estados n√£o nulos

data_1 = data_1[data_1['Regi√£o'].notnull()]
data_1.head()


# # 0.4) Informa√ß√£o e remo√ß√£o texto nas colunas 
# 
# **AIH - Aprovadas no per√≠odo sem considerar prorroga√ß√£o**
# - Uma parte importante para interna√ß√£o hospitalar.

# In[135]:


# Remo√ß√£o de pontos 

data_1 = data_1[data_1['Regi√£o'].str.contains('.', regex=False)]
data_1['Regi√£o'].unique()


# In[136]:


# Uma limpeza na coluna "Regi√£o"

data_1['Regi√£o'] = data_1['Regi√£o'].apply(lambda x: x.replace('.',''))
data_1['Regi√£o'] = data_1['Regi√£o'].apply(lambda x: x.lstrip())
data_1['Regi√£o'] = data_1['Regi√£o'].apply(lambda x: x.rstrip())

estados_df = {
    'AC': 'Acre',
    'AL': 'Alagoas',
    'AP': 'Amap√°',
    'AM': 'Amazonas',
    'BA': 'Bahia',
    'CE': 'Cear√°',
    'DF': 'Distrito Federal',
    'ES': 'Esp√≠rito Santo',
    'GO': 'Goi√°s',
    'MA': 'Maranh√£o',
    'MT': 'Mato Grosso',
    'MS': 'Mato Grosso do Sul',
    'MG': 'Minas Gerais',
    'PA': 'Par√°',
    'PB': 'Para√≠ba',
    'PR': 'Paran√°',
    'PE': 'Pernambuco',
    'PI': 'Piau√≠',
    'RJ': 'Rio de Janeiro',
    'RN': 'Rio Grande do Norte',
    'RS': 'Rio Grande do Sul',
    'RO': 'Rond√¥nia',
    'RR': 'Roraima',
    'SC': 'Santa Catarina',
    'SP': 'S√£o Paulo',
    'SE': 'Sergipe',
    'TO': 'Tocantins'
}

df_estados = {v: k for k, v in estados_df.items()}
data_1['Regi√£o'] = data_1['Regi√£o'].map(df_estados)

for i in data_1.columns:
    data_1[data_1[i] == '-'] = data_1[data_1[i] == '-'].apply(lambda x: x.replace('-', np.NaN))

# Separando m√™s e ano nos dados

data_1['mes'] = data_1['Data'].apply(lambda x: x[0:3])
data_1['ano'] = data_1['Data'].apply(lambda x: x[-2:])

# Nessa etapa substituindo meses extensos

meses = {'jan':'1', 
         'fev':'2', 
         'mar':'3', 
         'abr':'4', 
         'mai':'5', 
         'jun':'6', 
         'jul':'7', 
         'ago':'8', 
         'set':'9', 
         'out':'10', 
         'nov':'11', 
         'dez':'12'}

for k,v in meses.items():
    data_1['mes'] = data_1['mes'].apply(lambda x: x.replace(k,v))
    
# Transformando dados ano para 4 d√≠gitos

data_1['ano'] = data_1['ano'].apply(lambda x: '20'+x)

# Os dados em datas

data_1["data"] = data_1["ano"] + "-" + data_1["mes"]

# Visualizando o dataset completo

data_1.head()


# # 0.5) - Limpeza da base de dados
# 
# - Alguns dados tinha dados ausentes e nulos dentro do dataset.

# In[137]:


# Limpando a base de dados

data_1.drop(columns=["Val_serv_hosp_compl_federal", 
                     "Val_serv_hosp_compl_gestor", 
                     "Val_serv_prof_compl_federal",
                     "Val_serv_prof_compl_gestor",
                     "Data"], inplace = True)
data_1.head()


# In[138]:


# Salvando o dataset para modelo 2

data_1.to_csv('data1.csv', index=False)


# In[139]:


# Convertendo os dados para tipo datetime

data_1['data'] = pd.to_datetime(data_1['data'], format='%Y-%m')
data_1.info()


# In[140]:


# Dados faltantes

data_1.fillna(0, inplace=True)
data_1.head()


# In[141]:


# Per√≠odos faltantes

sorted(data_1['data'].unique())


# # 0.6) Estat√≠stica descritiva

# In[142]:


# Exibindo estat√≠sticas descritivas visualizar alguns detalhes estat√≠sticos b√°sicos como percentil, m√©dia, padr√£o, etc. 
# De um quadro de dados ou uma s√©rie de valores num√©ricos.

data_1.describe().T


# # 6.1) Gr√°fico de distribui√ß√£o normal

# In[143]:


# Gr√°fico distribui√ß√£o normal
plt.figure(figsize=(18.2, 8))

ax = sns.distplot(data_1['Taxa_mortalidade']);
plt.title("Distribui√ß√£o normal", fontsize=20)
plt.xlabel("Total de mortalidade")
plt.ylabel("Total")
plt.axvline(data_1['Taxa_mortalidade'].mean(), color='b')
plt.axvline(data_1['Taxa_mortalidade'].median(), color='r')
plt.axvline(data_1['Taxa_mortalidade'].mode()[0], color='g');
plt.legend(["Media", "Mediana", "Moda"])
plt.show()


# In[144]:


# Verificando os dados no boxplot regi√£o valor total verificando poss√≠veis outliers

ax = sns.boxplot(x="Regi√£o", y="Valor_total", data = data_1)
plt.title("Gr√°fico de boxplot - Regi√£o o valor total")
plt.xlabel("Total")
plt.ylabel("Valor total")


# In[145]:


# C√°lculo da m√©dia de interna√ß√µes e √≥bitos

media_interna√ß√µes = data_1[['data', 'Interna√ß√µes']].groupby('data').mean()
media_obitos = data_1[["data", "√ìbitos"]].groupby('data').mean()

print("M√©dia de m√©dia interna√ß√µes", media_interna√ß√µes)
print()
print("M√©dia de m√©dia √≥bitos", media_obitos)


# In[146]:


# Verifica√ß√£o m√©dia m√≥vel de interna√ß√µes e √≥bitos

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(50.5, 25));
plt.rcParams['font.size'] = '25'

ax1.plot(media_interna√ß√µes, marker='o', color = 'blue', markersize = 15);
ax1.set(title="M√©dia m√≥vel - Interna√ß√µes", xlabel = "Anos", ylabel = "Total de interna√ß√µes")
ax2.plot(media_obitos, marker='o', color = 'blue', markersize = 15);
ax2.set(title="M√©dia m√≥vel - √ìbitos", xlabel="Anos", ylabel="Total de √≥bitos")


# # 6.2) Matriz de correla√ß√£o dos dados

# In[147]:


# Matriz correla√ß√£o de pares de colunas, excluindo NA / valores nulos.

corr = data_1.corr()
corr


# In[148]:


# Gr√°fico da matriz de correla√ß√£o

plt.figure(figsize=(20,11))
ax = sns.heatmap(data_1.corr(), annot=True, cmap='YlGnBu');
plt.title("Matriz de correla√ß√£o")


# # 6.3) An√°lise de dados
# 
# - 2.1 - An√°lise 

# In[149]:


# Verificando √≥bitos por ano com gr√°fico interativo 
fig = px.bar(data_1, x='ano', y='√ìbitos', title='√ìbitos por ano')
fig.show()


# In[150]:


# Observando total de interna√ß√µes

sns.barplot(x='Valor_total', y='Regi√£o', data=data_1)
plt.title("Total de interna√ß√µes do SUS por regi√£o")
plt.xlabel("Regi√£o os estados")
plt.ylabel("Total")


# In[151]:


# Observando total de interna√ß√µes

sns.histplot(data_1["Interna√ß√µes"])
plt.title("Interna√ß√µes na UTI")
plt.xlabel("Interna√ß√µes")
plt.ylabel("Total")


# In[152]:


# Observando total de √≥bitos

sns.histplot(data_1["√ìbitos"])
plt.title("Total de √≥bitos")
plt.xlabel("√ìbitos")
plt.ylabel("Total")


# In[153]:


# Observando m√©dia do valor do AIH

sns.histplot(data_1["Valor_m√©dio_AIH"])
plt.title("Valor m√©dio AIH")
plt.xlabel("M√©dio AIH")
plt.ylabel("Total")


# In[154]:


# Observando m√©dia de interna√ß√µes

sns.histplot(data_1["Valor_m√©dio_intern"])
plt.title("Valor total de interna√ß√µes")
plt.xlabel("M√©dio interna√ß√µes")
plt.ylabel("Total")


# In[155]:


# Observando total da taxa de mortalidade

sns.histplot(data_1["Taxa_mortalidade"])
plt.title("Valor total taxa mortalidade")
plt.xlabel("M√©dio interna√ß√µes")
plt.ylabel("Total")


# In[156]:


# Comparando perman√™ncia m√©dia da UTI

sns.histplot(data_1["M√©dia_perman√™ncia"])
plt.title("Valor total da m√©dia perman√™ncia na UTI")
plt.xlabel("M√©dio m√©dia perman√™ncia")
plt.ylabel("Total")


# In[157]:


# Comparando o AIH de aprovados

sns.histplot(data_1["AIH_aprovadas"])
plt.title("Valor total da AIH aprovados")
plt.xlabel("AIH aprovados")
plt.ylabel("Total")


# # 6.4) An√°lise de dados = Univariada

# In[158]:


# Fazendo um comparativo dos dados 

data_1.hist(bins = 40, figsize=(50.2, 20))
plt.title("Gr√°fico de histograma")
plt.show()


# # 6.5) Data Processing
# 
# **O processamento de dados come√ßa com os dados em sua forma bruta e os converte em um formato mais leg√≠vel (gr√°ficos, documentos, etc.), dando-lhes a forma e o contexto necess√°rios para serem interpretados por computadores e utilizados.**
# 
# - Exemplo: Uma letra, um valor num√©rico. Quando os dados s√£o vistos dentro de um contexto e transmite algum significado, tornam-se informa√ß√µes.

# In[159]:


# Limpeza dos dados
data_1.drop(columns=["Regi√£o", "data"], inplace = True)
data_1.head()


# In[160]:


# Mundando os tipo de dados de object para inteiros 

data_1['√ìbitos'] = data_1['√ìbitos'].astype(int)
data_1['Taxa_mortalidade'] = data_1['Taxa_mortalidade'].astype(int)
data_1['Interna√ß√µes'] = data_1['Interna√ß√µes'].astype(int)
data_1.info()


# # 6.6) Feature Engineering
# 
# - Praticamente todos os algoritmos de Aprendizado de M√°quina possuem entradas e sa√≠das. As entradas s√£o formadas por colunas de dados estruturados, onde cada coluna recebe o nome de feature, tamb√©m conhecido como vari√°veis independentes ou atributos. Essas features podem ser palavras, peda√ßos de informa√ß√£o de uma imagem, etc. Os modelos de aprendizado de m√°quina utilizam esses recursos para classificar as informa√ß√µes. 
# 
# **Por exemplo, sedentarismo e fator heredit√°rio s√£o vari√°veis independentes para quando se quer prever se algu√©m vai ter c√¢ncer ou n√£o**  
# 
# - As sa√≠das, por sua vez, s√£o chamadas de vari√°veis dependentes ou classe, e essa √© a vari√°vel que estamos tentando prever. O nosso resultado pode ser 0 e 1 correspondendo a 'N√£o' e 'Sim' respectivamente, que responde a uma pergunta como: "Fulano √© bom pagador?" ou a probabilidade de algu√©m comprar um produto ou n√£o.

# In[161]:


# Importando a biblioteca para pr√©-processamento 

from sklearn.preprocessing import LabelEncoder

for i in data_1.columns:
    if data_1[i].dtype==np.number:
        continue
    data_1[i]= LabelEncoder().fit_transform(data_1[i])
    
data_1.head(4)


# # 6.7) Treino e Teste
# 
# - Treino e teste da base de dados da coluna Interna√ß√µes

# In[162]:


y = data_1['Interna√ß√µes'] # Vari√°vel para teste
x = data_1.drop('Interna√ß√µes', axis=1) # Vari√°vel para treino


# In[163]:


# Total de linhas e colunas dados vari√°vel x
x.shape


# In[164]:


# Total de linhas e colunas dados vari√°vel y
y.shape


# # 6.8) - Escalonamento
# 
# - Escalonamento uma forma de contornar os problemas relacionados √† escala, mantendo a informa√ß√£o estat√≠stica dos dados. O procedimento consiste em realizar uma transforma√ß√£o sobre o conjunto original dos dados de modo que cada vari√°vel apresente m√©dia zero e vari√¢ncia unit√°ria.

# In[165]:


# Importando a biblioteca sklearn para o escalonamneto dos dados

from sklearn.preprocessing import StandardScaler 

scaler_pre = StandardScaler() # Inicializando o escalonamento
scaler_pre_fit_train = scaler_pre.fit_transform(x) # Treinamento com a fun√ß√£o fit_transform com a vari√°vel x
scaler_pre_fit_train # Imprimindo o valor do escalonamento


# # 6.9) Modelo treinado para x, y valor
# 
# - 20% para os dados de treino
# - 80% para teste
# - Random state igual a zero

# In[166]:


# Importa√ß√£o da biblioteca sklearn para treino e teste do modelo

from sklearn.model_selection import train_test_split 

x_train, x_test, y_train, y_test = train_test_split(x, # Vari√°vel x
                                                    y, # Vari√°vel y
                                                    test_size=0.2, # Divivindo os dados em 20% para treino e 80% para teste
                                                    random_state = 0) # Random state igual a zero


# In[167]:


# Total de linhas e colunas e linhas dos dados de treino x

x_train.shape


# In[168]:


# Total de linhas dos dados de treino y

y_train.shape


# In[169]:


# Total de linhas e colunas dos dados de treino x teste 

x_test.shape


# In[170]:


# Total de linhas e colunas dos dados de treino y teste 

y_test.shape


# # 7.0) Modelo machine learning 
# 
# Eu utlizei modelo de regress√£o linear para prever interna√ß√µes, √≥bitos e valor m√©dio de AIH.

# # 7.1) Modelo 01 - Regress√£o linear
# 
# - Nesse modelo estamos prevendo o n√∫mero de interna√ß√µes utilizando modelo de regress√£o linear.

# In[171]:


# Modelo regress√£o linear - 1
# Importa√ß√£o da biblioteca sklearn o modelo regress√£o linear

from sklearn.linear_model import LinearRegression 

# Nome do algoritmo M.L
model_linear = LinearRegression() 

# Treinamento do modelo
model_linear_fit = model_linear.fit(x_train, y_train)

# Score do modelo
model_linear_score_1 = model_linear.score(x_train, y_train)

print("Modelo - Regress√£o linear: %.2f" % (model_linear_score_1 * 100))


# In[172]:


# Previs√£o do modelo

model_linear_pred = model_linear.predict(x_test)
model_linear_pred


# In[173]:


# O intercepto representa o efeito m√©dio em tendo todas as vari√°veis explicativas exclu√≠das do modelo. 
# De forma mais simples, o intercepto representa o efeito m√©dio em s√£o iguais a zero.

model_linear.intercept_


# In[174]:


# Os coeficientes de regress√£o  ùõΩ2 ,  ùõΩ3  e  ùõΩ4  s√£o conhecidos como coeficientes parciais de regress√£o ou coeficientes parciais angulares. 
# Considerando o n√∫mero de vari√°veis explicativas de nosso modelo, seu significado seria o seguinte

model_linear.coef_


# In[175]:


# O coeficiente de determina√ß√£o (R¬≤) √© uma medida resumida que diz quanto a linha de regress√£o ajusta-se aos dados. 
# √â um valor entra 0 e 1.

print('R¬≤ = {}'.format(model_linear.score(x_train, y_train).round(2)))


# In[176]:


# Previs√£o do modelo 
pred = model_linear.predict(x_train)
pred2 = y_train - pred
pred2


# # Gr√°fico de regress√£o linear

# In[177]:


# Grafico de regress√£o linear

plt.figure(figsize=(18, 8))
plt.scatter(pred, y_train)
plt.plot(pred, model_linear.predict(x_train), color = "red")
plt.title("Grafico de regress√£o linear", fontsize = 20)
plt.xlabel("Total")
plt.ylabel("Totoal de interna√ß√µes")
plt.legend(["Valor", "Interna√ß√µes"])


# # 7.2) Distribui√ß√£o de Frequ√™ncias dos Res√≠duos

# In[178]:


# Gr√°fico de distribui√ß√£o Frequ√™ncias

ax = sns.distplot(pred)
ax.figure.set_size_inches(20, 8)
ax.set_title('Distribui√ß√£o de Frequ√™ncias dos Res√≠duos', fontsize=18)
ax.set_xlabel('Interna√ß√µes', fontsize=14)
ax


# # 7.3) M√©tricas para o modelo de regress√£o linear

# - RMSE: Raiz do erro quadr√°tico m√©dio 
# - MAE: Erro absoluto m√©dio  
# - MSE: Erro m√©dio quadr√°tico
# - MAPE: Erro Percentual Absoluto M√©dio
# - R2: O R-Quadrado, ou Coeficiente de Determina√ß√£o, √© uma m√©trica que visa expressar a quantidade da varian√ßa dos dados.

# In[179]:


# Importando bibliotecas verifica√ß√µes das m√©tricas 

from math import sqrt
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

rmse = np.sqrt(mean_squared_error(y_test, model_linear_pred))
mae = mean_absolute_error(y_test, model_linear_pred)
mape = mean_absolute_percentage_error(y_test, model_linear_pred)
mse = mean_squared_error(y_test, model_linear_pred)
r2 = r2_score(y_test, model_linear_pred)

pd.DataFrame([rmse, mae, mse, mape, r2], ['RMSE', 'MAE', 'MSE', "MAPE",'R¬≤'], columns=['Resultado'])


# In[222]:


# Previs√£o de interna√ß√µes

prev = x_test[0:25]
model_pred = model_linear.predict(prev)[0]
print("Previs√£o de interna√ß√µes", model_pred)
prev


# # 7.4) Modelo 02
# 
# - Nesse segundo modelo estamos prevendo o n√∫mero de √≥bitos utilizando modelo de regress√£o linear.

# In[181]:


# Criando uma Series (pandas) para armazenar n√∫meros de √≥bitos

test = data_1['√ìbitos'] # Vari√°vel para teste
train = data_1.drop('√ìbitos', axis=1) # Vari√°vel para treino


# In[182]:


# Total de linhas e colunas dados vari√°vel train

train.shape


# In[183]:


# Total de linhas e colunas dados vari√°vel test

test.shape


# # 7.5) Escalonamento dos dados

# In[184]:


# Importando a biblioteca sklearn para o escalonamneto dos dados

from sklearn.preprocessing import StandardScaler

scaler_pre = StandardScaler() # Inicializando o escalonamento
scaler_pre_fit_train = scaler_pre.fit_transform(train) # Treinamento com a fun√ß√£o fit_transform com a vari√°vel x
scaler_pre_fit_train # Imprimindo o valor do escalonamento


# # 7.6) Modelo treinado para x, y valor
# 
# - 20% para os dados de treino
# - 80% para teste
# - Random state igual a zero

# In[185]:


# Importa√ß√£o da biblioteca sklearn para treino e teste do modelo

from sklearn.model_selection import train_test_split 

x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(train, # Vari√°vel train
                                                    test, # Vari√°vel test
                                                    test_size=0.2, # Divivindo os dados em 20% para treino e 80% para teste
                                                    random_state = 0) # Random state igual a zero


# In[186]:


# Total de linhas e colunas e linhas dos dados de treino x

x_train_1.shape


# In[187]:


# Total de linhas dos dados de treino y

y_train_1.shape


# In[188]:


# Total de linhas e colunas dos dados de treino x teste 

x_test_1.shape


# In[189]:


# Total de linhas e colunas dos dados de treino y teste 

y_test_1.shape


# # 7.7) Modelo regress√£o linear - 2

# In[190]:


# Modelo regress√£o linear - 2 √ìbitos
# Importa√ß√£o da biblioteca sklearn o modelo regress√£o linear

from sklearn.linear_model import LinearRegression

# Nome do algoritmo M.L
model_linear_2 = LinearRegression() 

# Treinamento do modelo
model_linear_fit = model_linear_2.fit(x_train_1, y_train_1)

# Score do modelo
model_linear_score_2 = model_linear_2.score(x_train_1, y_train_1)

print("Modelo - Regress√£o linear: %.2f" % (model_linear_score_2 * 100))


# In[191]:


# Previs√£o do modelo

model_linear_pred_2 = model_linear_2.predict(x_test_1)
model_linear_pred_2


# In[192]:


# O intercepto representa o efeito m√©dio em tendo todas as vari√°veis explicativas exclu√≠das do modelo. 
# De forma mais simples, o intercepto representa o efeito m√©dio em s√£o iguais a zero.

model_linear_2.intercept_


# In[193]:


# Os coeficientes de regress√£o  ùõΩ2 ,  ùõΩ3  e  ùõΩ4  s√£o conhecidos como coeficientes parciais de regress√£o ou coeficientes parciais angulares. 
# Considerando o n√∫mero de vari√°veis explicativas de nosso modelo, seu significado seria o seguinte

model_linear_2.coef_


# In[194]:


# O coeficiente de determina√ß√£o (R¬≤) √© uma medida resumida que diz quanto a linha de regress√£o ajusta-se aos dados. 
# √â um valor entra 0 e 1.

print('R¬≤ = {}'.format(model_linear_2.score(x_train_1, y_train_1).round(2)))


# In[195]:


# Previs√£o do modelo 

pred_2 = model_linear_2.predict(x_train_1)
pred_2 = y_train - pred_2
pred_2


# In[196]:


# Grafico de regress√£o linear

plt.figure(figsize=(18, 8))
plt.scatter(pred, y_train_1)
plt.title("Grafico de regress√£o linear - √ìbitos", fontsize = 20)
plt.xlabel("Total")
plt.ylabel("Totoal de √≥bitos")
plt.legend(["√ìbitos", "Valor"])


# In[197]:


# Distribui√ß√£o de Frequ√™ncias dos Res√≠duos

ax = sns.distplot(pred_2)
ax.figure.set_size_inches(20, 8)
ax.set_title('Distribui√ß√£o de Frequ√™ncias dos Res√≠duos', fontsize=18)
ax.set_xlabel('Interna√ß√µes', fontsize=14)
ax


# # 7.8) M√©tricas para o modelo 2 regress√£o linear
# 
# - RMSE: Raiz do erro quadr√°tico m√©dio 
# - MAE: Erro absoluto m√©dio  
# - MSE: Erro m√©dio quadr√°tico
# - MAPE: Erro Percentual Absoluto M√©dio
# - R2: O R-Quadrado, ou Coeficiente de Determina√ß√£o, √© uma m√©trica que visa expressar a quantidade da varian√ßa dos dados.

# In[198]:


# Verifica√ß√µes das m√©tricas 

rmse = np.sqrt(mean_squared_error(y_test, model_linear_pred_2))
mae = mean_absolute_error(y_test, model_linear_pred_2)
mape = mean_absolute_percentage_error(y_test, model_linear_pred_2)
mse = mean_squared_error(y_test, model_linear_pred_2)
r2 = r2_score(y_test, model_linear_pred_2)

pd.DataFrame([rmse, mae, mse, mape, r2], ['RMSE', 'MAE', 'MSE', "MAPE",'R¬≤'], columns=['Resultado'])


# In[221]:


# Previs√£o de obitos

prev_2 = x_test_1[0:25]
model_pred_2 = model_linear_2.predict(prev_2)[0]
print("Previs√£o de √≥bitos", model_pred_2)
prev_2


# # 7.9) Modelo 03: Regress√£o linear
# 
# - Nesse modelo estamos prevendo o valor M√©dio de AIH pelos pr√≥ximos 6 meses utilizando modelo de regress√£o linear.

# # Treino e Teste
# 
# - Treino e teste da base de dados da coluna Interna√ß√µes

# In[224]:


y2 = data_1['Valor_m√©dio_AIH'] # Vari√°vel para y2
x1 = data_1.drop('Valor_m√©dio_AIH', axis=1) # Vari√°vel para x1


# In[225]:


# Total de linhas e colunas dados vari√°vel x

x1.shape


# In[226]:


# Total de linhas e colunas dados vari√°vel y

y2.shape


# # 8.0) Escalonamento dos dados

# In[227]:


# Importando a biblioteca sklearn para o escalonamneto dos dados

from sklearn.preprocessing import StandardScaler 

scaler_pre = StandardScaler() # Inicializando o escalonamento
scaler_pre_fit_train = scaler_pre.fit_transform(x1) # Treinamento com a fun√ß√£o fit_transform com a vari√°vel x1
scaler_pre_fit_train # Imprimindo o valor do escalonamento


# # 9.0) Modelo treinado para x, y valor
# 
# - 20% para os dados de treino
# - 80% para teste
# - Random state igual a zero

# In[228]:


# Importa√ß√£o da biblioteca sklearn para treino e teste do modelo

from sklearn.model_selection import train_test_split 

train_x, test_x, train_y, test_y = train_test_split(train, # Vari√°vel x1
                                                    test, # Vari√°vel y2
                                                    test_size=0.2, # Divivindo os dados em 20% para treino e 80% para teste
                                                    random_state = 0) # Random state igual a zero


# In[229]:


# Total de linhas e colunas e linhas dos dados de treino x

train_x.shape


# In[230]:


# Total de linhas dos dados de treino y

train_y.shape


# In[231]:


# Total de linhas e colunas dos dados de treino x teste 

test_x.shape


# In[232]:


# Total de linhas e colunas dos dados de treino y teste 

test_y.shape


# In[233]:


# Modelo regress√£o linear - 3 Valor M√©dio de AIH
# Importa√ß√£o da biblioteca sklearn o modelo regress√£o linear

from sklearn.linear_model import LinearRegression

# Nome do algoritmo M.L
model_linear_3 = LinearRegression() 

# Treinamento do modelo
model_linear_fit = model_linear_3.fit(train_x, train_y)

# Score do modelo
model_linear_score_3 = model_linear_3.score(x_train_1, y_train_1)

print("Modelo - Regress√£o linear: %.2f" % (model_linear_score_3 * 100))


# In[234]:


# Previs√£o do modelo

model_linear_pred_3 = model_linear_3.predict(x_test_1)
model_linear_pred_3


# In[235]:


# O intercepto representa o efeito m√©dio em tendo todas as vari√°veis explicativas exclu√≠das do modelo. 
# De forma mais simples, o intercepto representa o efeito m√©dio em s√£o iguais a zero.

model_linear_3.intercept_


# In[236]:


# Os coeficientes de regress√£o  ùõΩ2 ,  ùõΩ3  e  ùõΩ4  s√£o conhecidos como coeficientes parciais de regress√£o ou coeficientes parciais angulares. 
# Considerando o n√∫mero de vari√°veis explicativas de nosso modelo, seu significado seria o seguinte.

model_linear_3.coef_


# In[237]:


# O coeficiente de determina√ß√£o (R¬≤) √© uma medida resumida que diz quanto a linha de regress√£o ajusta-se aos dados. 
# √â um valor entra 0 e 1.

print('R¬≤ = {}'.format(model_linear_3.score(x_train_1, y_train_1).round(2)))


# In[238]:


# Previs√£o do modelo

pred_2 = model_linear_3.predict(train_x)
pred_2 = y_train - pred_2
pred_2


# In[239]:


# Grafico de regress√£o linear

plt.figure(figsize=(18, 8))
plt.scatter(pred, train_y)
plt.title("Grafico de regress√£o linear", fontsize = 20)
plt.xlabel("Total")
plt.ylabel("Totoal de Valor M√©dio de AIH")
plt.legend(["Valor M√©dio de AIH", "Valor"])


# In[240]:


# Distribui√ß√£o de Frequ√™ncias dos Res√≠duos

ax = sns.distplot(pred_2)
ax.figure.set_size_inches(20, 8)
ax.set_title('Distribui√ß√£o de Frequ√™ncias dos Res√≠duos', fontsize=18)
ax.set_xlabel('Interna√ß√µes', fontsize=14)
ax


# # 1.0) M√©tricas para o modelo 3 regress√£o linear 
# 
# - RMSE: Raiz do erro quadr√°tico m√©dio 
# - MAE: Erro absoluto m√©dio  
# - MSE: Erro m√©dio quadr√°tico
# - MAPE: Erro Percentual Absoluto M√©dio
# - R2: O R-Quadrado, ou Coeficiente de Determina√ß√£o, √© uma m√©trica que visa expressar a quantidade da varian√ßa dos dados.

# In[241]:


# Verifica√ß√µes das m√©tricas 

rmse = np.sqrt(mean_squared_error(y_test, model_linear_pred_3))
mae = mean_absolute_error(y_test, model_linear_pred_3)
mape = mean_absolute_percentage_error(y_test, model_linear_pred_3)
mse = mean_squared_error(y_test, model_linear_pred_3)
r2 = r2_score(y_test, model_linear_pred_3)

pd.DataFrame([rmse, mae, mse, mape, r2], ['RMSE', 'MAE', 'MSE', "MAPE",'R¬≤'], columns=['Resultados'])


# In[242]:


# Previs√£o valor M√©dio de AIH

prev_3 = x_test_1[0:25]
model_pred_3 = model_linear_3.predict(prev_3)[0]
print("Previs√£o total valor M√©dio de AIH:", model_pred_3)
prev_3


# # 1.1) Resultados final dos modelos

# In[243]:


# Exibindo um comparativo dos modelos de regress√£o linear

modelos = pd.DataFrame({
    
    "Modelos" :[ "Modelo regress√£o linear 1", 
                "Modelo regress√£o linear 2", 
                "Modelo regress√£o linear 3"],

    "Acur√°cia" :[model_linear_score_1, 
                 model_linear_score_2, 
                 model_linear_score_3]})

modelos.sort_values(by = "Acur√°cia", ascending = True)


# In[244]:


# Salvando modelo de regress√£o linear

import pickle

with open('model_linear_pred.pkl', 'wb') as file:
    pickle.dump(model_linear_pred, file)
    
with open('model_linear_pred_2.pkl', 'wb') as file:
    pickle.dump(model_linear_pred_2, file)
    
with open('model_linear_pred_3.pkl', 'wb') as file:
    pickle.dump(model_linear_pred_3, file)


# # Conclus√£o do modelo machine learning

# Pela an√°lise dos modelos, modelo 1 teve melhor resultado que os demais, atigindo uma acur√°cia de 97.18% ou seja capaz de acertar as previs√µes de interna√ß√µes, √≥bitos, valor do AIH. De acordo com an√°lise realizada.  
